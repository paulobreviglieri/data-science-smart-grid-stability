{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Smart Grid Stability with Deep Learning\n",
    "### Release 1.0 - May, 2020\n",
    "### Paulo Breviglieri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">This notebook is based on the \"<b>Electrical Grid Stability Simulated Dataset</b>\", created by Vadim Arzamasov (Karlsruher Institut für Technologie, Karlsruhe, Germany) and donated to the <b>University of California (UCI) Machine Learning Repository</b> (link <a href=\"https://archive.ics.uci.edu/ml/datasets/Electrical+Grid+Stability+Simulated+Data+#\">here</a>), where it is currently hosted.</p>\n",
    "<p style=\"text-align: justify\">Two primary references support this machine learning exercise and demand special mention:</p>\n",
    "<ol>\n",
    "    <li>\"<em><b>Taming instabilities in power grid networks by decentralized control</b></em>\" (B. Schäfer, et al, The European Physical Journal, Special Topics, 2016, 225.3: 569-582), in which Dr. Schäfer (Network Dynamics, Max Planck Institute for Dynamics and Self-Organization - MPIDS, Göttingen, Germany) and his co-authors describe in detail the DSGC (Decentral Smart Grid Control) differential equation-based model to assess stability of smart grids;</li>\n",
    "    <p></p>\n",
    "    <li>\"<em><b>Towards Concise Models of Grid Stability</b></em>\" (V. Arzamasov, K. Böhm and P. Jochem, 2018 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm), Aalborg, 2018, pp. 1-6), in which Dr. Arzamasov and his co-authors explore how data-mining techniques can address DSGC model simplifications.</li>\n",
    "</ol>\n",
    "<p style=\"text-align: justify\">The author is particularly thankful for Dr. Arzamasov's personal guidance and comments on the overall dataset structure.</p>\n",
    "<p style=\"text-align: justify\">Logic enhancement and code forking are welcome and encouraged provided that this work is properly referenced. Thank you.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Renewable Energy Sources and Smart Grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The ascent of renewable energy sources provides the global community with a much demanded alternative to traditional, finite and climate-unfriendly fossil fuels. However, their adoption poses a set of new paradigms, out of which two interrelated aspects deserve particular attention:</p>\n",
    "<ul>\n",
    "    <li style=\"text-align: justify\">Prior to the rise of renewable energy sources, the traditional operating ecosystem involved few production entities (sources) supplying energy to consumers over unidirectional flows. With the advent of renewable options, end users (households and enterprises) now not only consume energy but have the ability to produce and supply it - hence a new term to designate them, '<b>prosumers</b>'. As a result, energy flow within distribution grids - '<b>smart grids</b>' - has become <b>bidirectional</b>;</li>\n",
    "    <li style=\"text-align: justify\">Despite the increased flexibility brought in by the introduction of renewable sources and the aforementioned emergence of 'prosumers', the management of supply and demand in a more complex generation / distribution / consumption environment and the related economic implications (particularly the decision to buy energy at a given price or not) have become even more challenging.</li>\n",
    "</ul>\n",
    "<p style=\"text-align: justify\">Relevant contributions on how to tackle the requirements of such new scenario have been offered by academy and industry over the past years. Special attention has been devoted to the study of <b>smart grid stability</b>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Modeling grid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">In a smart grid, consumer demand information is collected, centrally evaluated against current supply conditions and the resulting proposed price information is sent back to customers for them to decide about usage. As the whole process is time-dependent, dinamically estimating <b>grid stability</b> becomes not only a concern but a major requirement.</p>\n",
    "<p style=\"text-align: justify\">Put simply, the objective is to understand and plan for both energy production and/or consumption disturbances and fluctuations introduced by system participants in a dynamic way, taking into consideration not only technical aspects but also how participants respond to changes in the associated economic aspects (energy price).</p>\n",
    "<p style=\"text-align: justify\">The work of researchers cited in foreword focuses on <b>Decentral Smart Grid Control</b> (DSGC) systems, a methodology strictly tied to monitoring one particular property of the grid - its frequency.</p>\n",
    "<p style=\"text-align: justify\">The term '<em>frequency</em>' refers to the alternate current (AC) frequency, measured in cycles per second or its equivalent unit, Hertz (Hz). Around the world AC frequencies of either 50 or 60 Hz are utilized in electric power generation-distribution systems.</p>\n",
    "<p style=\"text-align: justify\">It is known that the electrical signal frequency \"<em>increases in times of excess generation, while it decreases in times of underproduction</em>\" [1]. Therefore, <b>measuring the grid frequency</b> at the premise of each customer would suffice to provide the network administrator with all required information about the current <b>network power balance</b>, so that it can price its energy offering - and inform consumers - accordingly.</p>\n",
    "<p style=\"text-align: justify\">The DSGC differential equation-based mathematical model described in [1] and assessed in [2] aims at identifying grid instability for a reference <b>4-node star</b> architecture, comprising one power source (a centralized generation node) supplying energy to three consumption nodes. The model takes into consideration inputs (features) related to:\n",
    "    <ul>\n",
    "        <li>the total <b>power balance</b> (nominal power produced or consumed at each grid node);</li>\n",
    "        <li>the response time of participants to adjust consumption and/or production in response to price changes (referred to as \"<b>reaction time</b>);</li>\n",
    "        <li>energy <b>price elasticity</b>.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/hvmW0cg.png\" width=\"500\" height=\"100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Addressing simplifications in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">So we have a mathematical model with which grid instability can be predicted! The need of a tool to predict grid instability would have been met, and the binary classification (\"stable\" versus \"unstable\") problem would be solved! However, the execution of this model relies on significant <b>simplifications</b>.</p>\n",
    "<p style=\"text-align: justify\">A differential equation-based model can be manipulated in several ways. One traditional approach consists in running simulations with a combination of fixed values for one subset of variables and fixed value distributions for the remaining subset. As elegantly depicted in [2], this strategy leads to <b>two primary issues</b>, referred to as the \"fixed inputs issue\" and the \"equality issue\". Please refer to [2] for a comprehensive assessment of both issues.</p>\n",
    "<p style=\"text-align: justify\">Alternative approaches have been proposed to overcome the inherent DSGC model simplifications. In particular, Dr. Arzamasov's team at the KIT suggest the use of machine learning - <b>decision trees (CART)</b> - and space-filling designs to process results from simulations with different DSGC parameter configurations.</p>\n",
    "<p>In other words, machine learning is used in [2] in the following way:</p>\n",
    "<ol>\n",
    "    <li>A given set of input parameters (call it a 'vector') is fed into the original DSGC model;</li>\n",
    "    <li>The DSGC model process this vector and returns a binary output - the grid stability for that particular set of inputs ('stable' or 'unstable' - a binary classification!);</li>\n",
    "    <li>Steps 1 and 2 are executed 'n' times;</li>\n",
    "    <li>A large set of vectors and the respective outputs (stability or instability) is created.</li>\n",
    "</ol>\n",
    "<p>In summary, the original DSGC model was run to generate a set of inputs and outputs that a 'learning machine' can process and make predictions from!</p>\n",
    "<p>Per [2], accuracies of \"around 80%\" have been achieved with the CART-based learning machine.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Objectives of this machine learning exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Considering the nature of the problem to be investigated and the dataset properties (as described in Section 3 below), two major objectives are proposed:</p>\n",
    "<ol>\n",
    "    <li style=\"text-align: justify\">Pursue improvements in predictions with <b>deep learning</b> (Keras' Sequential model);</li>\n",
    "    <li style=\"text-align: justify\">Take the opportunity to assess the influence of deep learning architecture (number and size of hidden layers), number of epochs and the relevance of dataset augmentation.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The dataset chosen for this machine learning exercise has a synthetic nature and contains results from simulations of grid stability for a reference 4-node star network, as described in 1.2.</p>\n",
    "<p style=\"text-align: justify\">The original dataset contains 10,000 observations. As the reference grid is symetric, the dataset can be augmented in 3! (3 factorial) times, or 6 times, representing a permutation of the three consumers occupying three consumer nodes. The augmented version has then <b>60,000 observations</b>. It also contains <b>12 primary predictive features</b> and two dependent variables. </p>\n",
    "<p><b>Predictive features</b>:</p>\n",
    "<ol>\n",
    "    <li>'tau1' to 'tau4': the reaction time of each network participant, a real value within the range 0.5 to 10 ('tau1' corresponds to the supplier node, 'tau2' to 'tau4' to the consumer nodes);</li>\n",
    "    <li>'p1' to 'p4': nominal power produced (positive) or consumed (negative) by each network participant, a real value within the range -2.0 to -0.5 for consumers ('p2' to 'p4'). As the total power consumed equals the total power generated, p1 (supplier node) = - (p2 + p3 + p4);</li>\n",
    "    <li>'g1' to 'g4': price elasticity coefficient for each network participant, a real value within the range 0.05 to 1.00 ('g1' corresponds to the supplier node, 'g2' to 'g4' to the consumer nodes; 'g' stands for 'gamma');</li>\n",
    "</ol>\n",
    "<p><b>Dependent variables</b>:</p>\n",
    "<ol>\n",
    "    <li>'stab': the maximum real part of the characteristic differentia equation root (if positive, the system is linearly unstable; if negative, linearly stable);</li>\n",
    "    <li>'stabf': a categorical (binary) label ('stable' or 'unstable').</li>\n",
    "</ol>\n",
    "<p style=\"text-align: justify\">As there is a direct relationship between 'stab' and 'stabf' ('stabf' = 'stable' if 'stab' <= 0, 'unstable' otherwise), 'stab' will be dropped and 'stabf' will remain as the sole dependent variable.</p>\n",
    "<p style=\"text-align: justify\">As the dataset content comes from simulation exercises, there are no missing values. Also, all features are originally numerical, no feature coding is required. Such dataset properties allow for a direct jump to machine modeling without the need of data preprocessing or feature engineering.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Importing required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Along with traditional libraries imported for tensor manipulation, mathematical operations and graphics development, three scikit-learn modules (StandardScaler as a scaler, confusion_matrix as the model performance metric of choice and KFold as the cross validation engine) and two Keras deep learning objects (Sequential and Dense) are used in this exercise.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Customized functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Functions were developed to assist with graphical analysis of specific dataset elements (features or observations) and mapping correlation. Please refer to the respective docstrings below for details. Note that all function variable names, by coding principle, start with the \"f_\" string, allowing for containerized processing within the function execution environment, not affecting global variables.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assessment(f_data, f_y_feature, f_x_feature, f_index=-1):\n",
    "    \"\"\"\n",
    "    Develops and displays a histogram and a scatter plot for a dependent / independent variable pair from\n",
    "    a dataframe and, optionally, highlights a specific observation on the plot in a different color (red).\n",
    "    \n",
    "    Also optionally, if an independent feature is not informed, the scatterplot is not displayed.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    \n",
    "    f_data      Tensor containing the dependent / independent variable pair.\n",
    "                Pandas dataframe\n",
    "    f_y_feature Dependent variable designation.\n",
    "                String\n",
    "    f_x_feature Independent variable designation.\n",
    "                String\n",
    "    f_index     If greater or equal to zero, the observation denoted by f_index will be plotted in red.\n",
    "                Integer\n",
    "    \"\"\"\n",
    "    for f_row in f_data:\n",
    "        if f_index >= 0:\n",
    "            f_color = np.where(f_data[f_row].index == f_index,'r','g')\n",
    "            f_hue = None\n",
    "        else:\n",
    "            f_color = 'b'\n",
    "            f_hue = None\n",
    "    \n",
    "    f_fig, f_a = plt.subplots(1, 2, figsize=(16,4))\n",
    "    \n",
    "    f_chart1 = sns.distplot(f_data[f_x_feature], ax=f_a[0], kde=False, color='g')\n",
    "    f_chart1.set_xlabel(f_x_feature,fontsize=10)\n",
    "    \n",
    "    if f_index >= 0:\n",
    "        f_chart2 = plt.scatter(f_data[f_x_feature], f_data[f_y_feature], c=f_color, edgecolors='w')\n",
    "        f_chart2 = plt.xlabel(f_x_feature, fontsize=10)\n",
    "        f_chart2 = plt.ylabel(f_y_feature, fontsize=10)\n",
    "    else:\n",
    "        f_chart2 = sns.scatterplot(x=f_x_feature, y=f_y_feature, data=f_data, hue=f_hue, legend=False)\n",
    "        f_chart2.set_xlabel(f_x_feature,fontsize=10)\n",
    "        f_chart2.set_ylabel(f_y_feature,fontsize=10)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def correlation_map(f_data, f_feature, f_number):\n",
    "    \"\"\"\n",
    "    Develops and displays a heatmap plot referenced to a primary feature of a dataframe, highlighting\n",
    "    the correlation among the 'n' mostly correlated features of the dataframe.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    \n",
    "    f_data      Tensor containing all relevant features, including the primary.\n",
    "                Pandas dataframe\n",
    "    f_feature   The primary feature.\n",
    "                String\n",
    "    f_number    The number of features most correlated to the primary feature.\n",
    "                Integer\n",
    "    \"\"\"\n",
    "    f_most_correlated = f_data.corr().nlargest(f_number,f_feature)[f_feature].index\n",
    "    f_correlation = f_data[f_most_correlated].corr()\n",
    "    \n",
    "    f_mask = np.zeros_like(f_correlation)\n",
    "    f_mask[np.triu_indices_from(f_mask)] = True\n",
    "    with sns.axes_style(\"white\"):\n",
    "        f_fig, f_ax = plt.subplots(figsize=(20, 10))\n",
    "        sns.heatmap(f_correlation, mask=f_mask, vmin=-1, vmax=1, square=True,\n",
    "                    center=0, annot=True, annot_kws={\"size\": 8}, cmap=\"PRGn\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Importing required datasets into dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The augmented dataset (60,000 observations) is imported. The dependent variable is map encoded ('stable' replaced with 1, 'unstable' with 0). At last, the 60,000 observations are shuffled.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "start_time = datetime.now()\n",
    "\n",
    "data = pd.read_csv('smart_grid_stability_augmented.csv')\n",
    "\n",
    "map1 = {'unstable': 0, 'stable': 1}\n",
    "data['stabf'] = data['stabf'].replace(map1)\n",
    "\n",
    "data = data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">A glimpse at the dataset structure confirms observation shuffling and the close relationship between the two original dependent variables 'stab' and 'stabf'.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Feature assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Distribution patterns and the relationship with the 'stab' dependent variable is charted for each of the 12 dataset features.</p>\n",
    "<p style=\"text-align: justify\">As this data comes from simulations with predetermined fixed ranges for all features, as described in Section 3, distributions are pretty much uniform across the board, with the exception of 'p1' (absolute sum of 'p2', 'p3' and 'p4'), which follows a normal distribution (as expected) with a very small skew factor of -0.013.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    assessment(data, 'stab', column, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.p1.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The proportion of observations related to 'unstable' and 'stable' scenarios is mapped.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Split of \"unstable\" (0) and \"stable\" (1) observations in the original dataset:')\n",
    "print(data['stabf'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">It is important to verify the correlation between each numerical feature and the dependent variable, as well as correlation among numerical features leading to potential undesired colinearity. The heatmap below provides an overview of correlation between the dependent variable ('stabf') and the 12 numerical features. Note that also the alternative dependent variable ('stab') has been included just to give an idea of how correlated it is with 'stabf'. Such correlation is significant (-0.83), as it should be, which reinforces the decision to drop it, anticipated in Section 3. Also, correlation between 'p1' and its components 'p2', 'p3' and 'p4' is above average, as expected, but not as high o justify any removal. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation_map(data, 'stabf', 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Segregating train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">As anticipated, the features dataset will contain all 12 original predictive features, while the label dataset will contain only 'stabf' ('stab' is dropped here).</p>\n",
    "<p style=\"text-align: justify\">In addition, as the dataset has already been shuffled, the training set will receive the first 54,000 observations, while the testing set will accommodate the last 6,000.</p>\n",
    "<p style=\"text-align: justify\">Even considering that the dataset is large enough and well behaved, the percentage of 'stable' and 'unstable' observations is computed for both training and testing sets, just to make sure that the original dataset distribution is maintained after the split - which proved to be the case.</p>\n",
    "<p style=\"text-align: justify\">After splitting, Pandas dataframes and series are transformed into Numpy arrays for the remainder of the exercise.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :12]\n",
    "y = data.iloc[:, 13]\n",
    "\n",
    "X_training = X.iloc[:54000, :]\n",
    "y_training = y.iloc[:54000]\n",
    "\n",
    "X_testing = X.iloc[54000:, :]\n",
    "y_testing = y.iloc[54000:]\n",
    "\n",
    "ratio_training = y_training.value_counts(normalize=True)\n",
    "ratio_testing = y_testing.value_counts(normalize=True)\n",
    "ratio_training, ratio_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training = X_training.values\n",
    "y_training = y_training.values\n",
    "\n",
    "X_testing = X_testing.values\n",
    "y_testing = y_testing.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">In preparation for machine learning, scaling is performed based on (fitted to) the training set and applied (with the 'transform' method) to both training and testing sets.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_training = scaler.fit_transform(X_training)\n",
    "X_testing = scaler.transform(X_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The artificial neural network (ANN) architecture depicted below is the optimal one evaluated in this study. It reflects an sequential structure with:</p>\n",
    "<ul>\n",
    "    <li>one input layer (12 input nodes);</li>\n",
    "    <li>three hidden layers (24, 24 and 12 nodes, respectively);</li>\n",
    "    <li>one single-node output layer.</li>\n",
    "</ul>\n",
    "<p style=\"text-align: justify\">Alternative architectures were evaluated with variations of the code below. Their performance will be discussed in Section 7.</p>\n",
    "<p style=\"text-align: justify\">As features are numerical real numbers within ranges, the choice of 'relu' as the activation function for hidden layers seems straightforward. Similarly, as this is a logistic classification exercise, where the output is binary ('0' for 'unstable', '1' for 'stable', following the map coding used in Section 4.3), the choice of 'sigmoid' as activation for the output layers seems obvious.</p>\n",
    "<p style=\"text-align: justify\">Compilation with 'adam' as optimizer and 'binary_crossentropy' as the loss function follow the same logic. The fitting performance will be assessed using 'accuracy' as the metric of choice.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/Uu57iWd.png\" width=\"600\" height=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN initialization\n",
    "classifier = Sequential()\n",
    "\n",
    "# Input layer and first hidden layer\n",
    "classifier.add(Dense(units = 24, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\n",
    "\n",
    "# Second hidden layer\n",
    "classifier.add(Dense(units = 24, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Third hidden layer\n",
    "classifier.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Single-node output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# ANN compilation\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Even considering that data is well behaved and in general uniformly distributed, a cross-validation based fitting is proposed. KFold is the cross-validation engine selected, and 10 different validation sets will be utilized.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_round = 1\n",
    "\n",
    "for train_index, val_index in KFold(10, shuffle=True, random_state=10).split(X_training):\n",
    "    x_train, x_val = X_training[train_index], X_training[val_index]\n",
    "    y_train ,y_val = y_training[train_index], y_training[val_index]\n",
    "    classifier.fit(x_train, y_train, epochs=50)\n",
    "    print(f'\\nModel evaluation - Round {cross_val_round}: {classifier.evaluate(x_val, y_val)}\\n')\n",
    "    cross_val_round += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Predicting smart grid stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">After fitting the model to the training set, it is time to extract predictions for the testing set and segregate those above the 'threshold' of 0.5 ('unstable' cases below the threshold, 'stable' cases above it).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_testing)\n",
    "y_pred[y_pred <= 0.5] = 0\n",
    "y_pred[y_pred > 0.5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Classification performance - Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The segregation described in Section 6.3 allows for the construction of a confusion matrix.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_testing, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy per the confusion matrix: {(cm[0][0]+cm[1][1])/cm.sum()*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Alternative configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The architecture and the hyperparameters selected above led to the best prediction performance on the test set.</p>\n",
    "<p style=\"text-align: justify\">In addition, several other combinations were evaluated for both the original dataset with 1,000 observations and the augmented dataset with 6,000 observations. It is important to emphasize that in this comparative assessment <b>no shuffling</b> of any type, at any part of the exercise, was performed, so that the very same testing set was exposed to model after fitting for performance assessment.</p>\n",
    "<p style=\"text-align: justify\">The table below summarizes obtained results:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/HQ2wySQ.png\" width=\"1024\" height=\"203\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Discussion and Final Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Specific aspects of this deep learning exercise demand special attention:</p>\n",
    "<ol>\n",
    "    <li style=\"text-align: justify\">Deep learning proved to be an outstanding prediction tool for this particular application. Even considering that the dataset is well behaved and needed no significant preprocessing, the <b>high accuracies</b> obtained on the testing set confirm that a deep learning model may be safely considered. It would though be up to a smart grid operator to confirm if the accuracy level obtained with deep learning would suffice in practical terms (live network);</li>\n",
    "    <li style=\"text-align: justify\">As expected, more complex ANN architectures <b>performed better</b> than simpler ones;</li>\n",
    "    <li style=\"text-align: justify\">An <b>increased number of epochs</b> considered during fitting also plays a major role. It is evident that the more the model is exposed to the training set, the better the prediction accuracy;</li>\n",
    "    <li style=\"text-align: justify\">From a machine learning exercise perspective, the use of an <b>augmented dataset</b> with 6,000 observations contributed significantly to better results;</li>\n",
    "    <li style=\"text-align: justify\">It must be noted that input parameters utilized in the original DSGC simulations fall within predetermined ranges. As a follow-up step in the validation of this learning machine, it would be interesting to assess its performance using a new test set with observations obtained from simulations with input parameter values residing in other alternative ranges.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = datetime.now()\n",
    "\n",
    "print('\\nStart time', start_time)\n",
    "print('End time', end_time)\n",
    "print('Time elapsed', end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
